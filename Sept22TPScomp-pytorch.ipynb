{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f797bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae3aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd4b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Advanced Techniques</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Getting Started</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Recipe Book</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle for Kids: One Smart Goose</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>KaggleRama</td>\n",
       "      <td>Kaggle Advanced Techniques</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id        date  country       store                           product  \\\n",
       "0       0  2017-01-01  Belgium  KaggleMart        Kaggle Advanced Techniques   \n",
       "1       1  2017-01-01  Belgium  KaggleMart            Kaggle Getting Started   \n",
       "2       2  2017-01-01  Belgium  KaggleMart                Kaggle Recipe Book   \n",
       "3       3  2017-01-01  Belgium  KaggleMart  Kaggle for Kids: One Smart Goose   \n",
       "4       4  2017-01-01  Belgium  KaggleRama        Kaggle Advanced Techniques   \n",
       "\n",
       "   num_sold  \n",
       "0       663  \n",
       "1       615  \n",
       "2       480  \n",
       "3       710  \n",
       "4       240  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic imports for data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#import data\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "#check out the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e009784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>01/01/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>04/04/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>04/05/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>05/01/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>05/13/21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Holiday\n",
       "0  Belgium  01/01/21\n",
       "1  Belgium  04/04/21\n",
       "2  Belgium  04/05/21\n",
       "3  Belgium  05/01/21\n",
       "4  Belgium  05/13/21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'Belgium'\n",
    "public_holidays = pd.read_csv(\"../data/EUpublicholidays.csv\")\n",
    "public_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e13fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_holidays = public_holidays.where(public_holidays['Country']==country)\n",
    "country_holidays = country_holidays.drop(np.where(country_holidays['Country'].isnull())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4306ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_holidays['Holiday']=pd.to_datetime(country_holidays['Holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e0178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85588387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a412f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date']=pd.to_datetime(data['date'])\n",
    "data['holiday'] = data['date'].isin(country_holidays['Holiday'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f775caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1f9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead collect useful data from date\n",
    "data['weekday'] = data['date'].map(lambda x : x.dayofweek).astype('category')\n",
    "data['month'] = data['date'].map(lambda x : x.month).astype('category')\n",
    "data['monthday'] = data['date'].map(lambda x : x.day).astype('category')\n",
    "#year is the only continuous variable, it needs to be scaled\n",
    "data['year'] = data['date'].map(lambda x : x.year - 2016).astype(float)\n",
    "data['year'] = data['year'].map(lambda x : (x - 1)/(4-1))\n",
    "data['product'] = data['product'].astype('category')\n",
    "data['store'] = data['store'].astype('category')\n",
    "dataf = data.drop('row_id',axis=1).drop('date',axis=1)\n",
    "# dataf.drop('index',axis=1,inplace=True)\n",
    "\n",
    "# dataf.drop('num_sold',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "dataf = dataf.where(dataf['country']==country)\n",
    "dataf = dataf.drop(np.where(dataf['country'].isnull())[0])\n",
    "y = dataf['num_sold'].to_numpy().astype('float32').reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33062ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WeekdayTransformer = OneHotEncoder(sparse=False)\n",
    "MonthTransformer = OneHotEncoder(sparse=False)\n",
    "MonthDayTransformer = OneHotEncoder(sparse=False)\n",
    "ProductTransformer = OneHotEncoder(sparse=False)\n",
    "HolidayTransformer = OneHotEncoder(sparse=False)\n",
    "StoreTransformer = OneHotEncoder(sparse=False)\n",
    "\n",
    "#create separate transformers to make it easier to drop one of the dummy variables and avoid\n",
    "#multi-collinearity problems\n",
    "weekday = WeekdayTransformer.fit_transform(dataf[['weekday']])[:,:-1]\n",
    "month = MonthTransformer.fit_transform(dataf[['month']])[:,:-1]\n",
    "monthday = MonthTransformer.fit_transform(dataf[['monthday']])[:,:-1]\n",
    "product = ProductTransformer.fit_transform(dataf[['product']])[:,:-1]\n",
    "holiday = HolidayTransformer.fit_transform(dataf[['holiday']])[:,:-1]\n",
    "store = StoreTransformer.fit_transform(dataf[['store']])[:,:-1]\n",
    "\n",
    "\n",
    "# transforms = ENC.fit_transform(dataf[['weekday','month','monthday','product','store','holiday']])\n",
    "year = dataf['year'].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda0883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8051292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate((weekday,month,monthday,product,holiday,store,year,y),axis=1)\n",
    "training_data = full_data[:(len(full_data)-2000)]\n",
    "test_data = full_data[(len(full_data)-2000):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f00265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9688, 53)\n"
     ]
    }
   ],
   "source": [
    "x_train = training_data[:,:-1][:]\n",
    "y_train = training_data[:,-1:][:]\n",
    "x_test = test_data[:,:-1][:]\n",
    "y_test = test_data[:,-1:][:]\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97eb655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e148fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c25a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9688, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "594f5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetMaker:\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "training_data =  DataSetMaker(x_train,y_train)\n",
    "test_data = DataSetMaker(x_test,y_test)\n",
    "\n",
    "training_data_loader = DataLoader(training_data,batch_size=8,shuffle=True)\n",
    "test_data_loader = DataLoader(test_data,batch_size=8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74106c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons =256\n",
    "dropout = 0.2\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(53, neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(neurons, neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(neurons, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c1ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bbabdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccdaebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss(pred, y).item()\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee39a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 85053.054688  [    0/ 9688]\n",
      "loss: 12210.537109  [  800/ 9688]\n",
      "loss: 607.246155  [ 1600/ 9688]\n",
      "loss: 2388.253906  [ 2400/ 9688]\n",
      "loss: 2757.569092  [ 3200/ 9688]\n",
      "loss: 1750.170288  [ 4000/ 9688]\n",
      "loss: 1192.749023  [ 4800/ 9688]\n",
      "loss: 881.704346  [ 5600/ 9688]\n",
      "loss: 1435.794678  [ 6400/ 9688]\n",
      "loss: 11295.060547  [ 7200/ 9688]\n",
      "loss: 303.168457  [ 8000/ 9688]\n",
      "loss: 498.025604  [ 8800/ 9688]\n",
      "loss: 412.625000  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1454.994372 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1048.535889  [    0/ 9688]\n",
      "loss: 908.437256  [  800/ 9688]\n",
      "loss: 992.209106  [ 1600/ 9688]\n",
      "loss: 470.613220  [ 2400/ 9688]\n",
      "loss: 1151.277344  [ 3200/ 9688]\n",
      "loss: 1533.388916  [ 4000/ 9688]\n",
      "loss: 338.242767  [ 4800/ 9688]\n",
      "loss: 587.309387  [ 5600/ 9688]\n",
      "loss: 542.048279  [ 6400/ 9688]\n",
      "loss: 719.423767  [ 7200/ 9688]\n",
      "loss: 2875.412354  [ 8000/ 9688]\n",
      "loss: 4509.156738  [ 8800/ 9688]\n",
      "loss: 201.530334  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1348.203009 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 874.090942  [    0/ 9688]\n",
      "loss: 112.182526  [  800/ 9688]\n",
      "loss: 1198.098145  [ 1600/ 9688]\n",
      "loss: 1172.175293  [ 2400/ 9688]\n",
      "loss: 90.763901  [ 3200/ 9688]\n",
      "loss: 4956.745605  [ 4000/ 9688]\n",
      "loss: 962.433167  [ 4800/ 9688]\n",
      "loss: 412.363281  [ 5600/ 9688]\n",
      "loss: 1117.580688  [ 6400/ 9688]\n",
      "loss: 1159.393799  [ 7200/ 9688]\n",
      "loss: 1515.571655  [ 8000/ 9688]\n",
      "loss: 357.550934  [ 8800/ 9688]\n",
      "loss: 653.285217  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1947.619460 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 635.167786  [    0/ 9688]\n",
      "loss: 1336.685913  [  800/ 9688]\n",
      "loss: 4167.058105  [ 1600/ 9688]\n",
      "loss: 795.213074  [ 2400/ 9688]\n",
      "loss: 1800.457886  [ 3200/ 9688]\n",
      "loss: 448.234894  [ 4000/ 9688]\n",
      "loss: 794.609436  [ 4800/ 9688]\n",
      "loss: 281.824341  [ 5600/ 9688]\n",
      "loss: 662.760864  [ 6400/ 9688]\n",
      "loss: 966.133484  [ 7200/ 9688]\n",
      "loss: 531.967163  [ 8000/ 9688]\n",
      "loss: 6758.687988  [ 8800/ 9688]\n",
      "loss: 2327.716797  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1215.209819 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 822.116882  [    0/ 9688]\n",
      "loss: 1764.868652  [  800/ 9688]\n",
      "loss: 549.477051  [ 1600/ 9688]\n",
      "loss: 739.163757  [ 2400/ 9688]\n",
      "loss: 1083.240356  [ 3200/ 9688]\n",
      "loss: 412.718201  [ 4000/ 9688]\n",
      "loss: 129.600388  [ 4800/ 9688]\n",
      "loss: 195.499115  [ 5600/ 9688]\n",
      "loss: 48.970070  [ 6400/ 9688]\n",
      "loss: 454.832184  [ 7200/ 9688]\n",
      "loss: 921.517395  [ 8000/ 9688]\n",
      "loss: 427.991394  [ 8800/ 9688]\n",
      "loss: 862.206909  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1087.186773 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 178.344315  [    0/ 9688]\n",
      "loss: 313.130524  [  800/ 9688]\n",
      "loss: 161.130920  [ 1600/ 9688]\n",
      "loss: 654.758667  [ 2400/ 9688]\n",
      "loss: 900.526855  [ 3200/ 9688]\n",
      "loss: 991.285645  [ 4000/ 9688]\n",
      "loss: 379.331940  [ 4800/ 9688]\n",
      "loss: 172.825912  [ 5600/ 9688]\n",
      "loss: 284.054718  [ 6400/ 9688]\n",
      "loss: 553.139099  [ 7200/ 9688]\n",
      "loss: 2851.215576  [ 8000/ 9688]\n",
      "loss: 823.280701  [ 8800/ 9688]\n",
      "loss: 183.756195  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1019.312560 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 764.297363  [    0/ 9688]\n",
      "loss: 529.712524  [  800/ 9688]\n",
      "loss: 74.973335  [ 1600/ 9688]\n",
      "loss: 1729.598267  [ 2400/ 9688]\n",
      "loss: 390.145874  [ 3200/ 9688]\n",
      "loss: 406.370667  [ 4000/ 9688]\n",
      "loss: 259.231689  [ 4800/ 9688]\n",
      "loss: 1129.314819  [ 5600/ 9688]\n",
      "loss: 528.980835  [ 6400/ 9688]\n",
      "loss: 325.989929  [ 7200/ 9688]\n",
      "loss: 418.777100  [ 8000/ 9688]\n",
      "loss: 57.303448  [ 8800/ 9688]\n",
      "loss: 110.424118  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1344.541383 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 319.663239  [    0/ 9688]\n",
      "loss: 615.968018  [  800/ 9688]\n",
      "loss: 520.494446  [ 1600/ 9688]\n",
      "loss: 704.359680  [ 2400/ 9688]\n",
      "loss: 889.942261  [ 3200/ 9688]\n",
      "loss: 561.890320  [ 4000/ 9688]\n",
      "loss: 309.948608  [ 4800/ 9688]\n",
      "loss: 129.516968  [ 5600/ 9688]\n",
      "loss: 922.522400  [ 6400/ 9688]\n",
      "loss: 278.625092  [ 7200/ 9688]\n",
      "loss: 192.236252  [ 8000/ 9688]\n",
      "loss: 168.641693  [ 8800/ 9688]\n",
      "loss: 715.590027  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1583.511769 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1589.277832  [    0/ 9688]\n",
      "loss: 512.656494  [  800/ 9688]\n",
      "loss: 1241.649414  [ 1600/ 9688]\n",
      "loss: 1157.383301  [ 2400/ 9688]\n",
      "loss: 464.898560  [ 3200/ 9688]\n",
      "loss: 418.175598  [ 4000/ 9688]\n",
      "loss: 406.978790  [ 4800/ 9688]\n",
      "loss: 728.770813  [ 5600/ 9688]\n",
      "loss: 1221.129272  [ 6400/ 9688]\n",
      "loss: 704.947510  [ 7200/ 9688]\n",
      "loss: 201.797165  [ 8000/ 9688]\n",
      "loss: 1098.981934  [ 8800/ 9688]\n",
      "loss: 839.537231  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1157.836360 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 222.548721  [    0/ 9688]\n",
      "loss: 591.650391  [  800/ 9688]\n",
      "loss: 2681.414062  [ 1600/ 9688]\n",
      "loss: 270.008179  [ 2400/ 9688]\n",
      "loss: 175.613495  [ 3200/ 9688]\n",
      "loss: 358.733093  [ 4000/ 9688]\n",
      "loss: 65.417572  [ 4800/ 9688]\n",
      "loss: 992.176514  [ 5600/ 9688]\n",
      "loss: 245.046738  [ 6400/ 9688]\n",
      "loss: 563.979309  [ 7200/ 9688]\n",
      "loss: 835.762451  [ 8000/ 9688]\n",
      "loss: 214.839645  [ 8800/ 9688]\n",
      "loss: 137.407532  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 980.794255 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 147.121597  [    0/ 9688]\n",
      "loss: 296.323975  [  800/ 9688]\n",
      "loss: 161.498337  [ 1600/ 9688]\n",
      "loss: 343.596130  [ 2400/ 9688]\n",
      "loss: 229.358597  [ 3200/ 9688]\n",
      "loss: 504.477448  [ 4000/ 9688]\n",
      "loss: 396.062714  [ 4800/ 9688]\n",
      "loss: 510.723236  [ 5600/ 9688]\n",
      "loss: 1040.228516  [ 6400/ 9688]\n",
      "loss: 200.457733  [ 7200/ 9688]\n",
      "loss: 210.899185  [ 8000/ 9688]\n",
      "loss: 115.760391  [ 8800/ 9688]\n",
      "loss: 640.883484  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 973.032256 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1370.751221  [    0/ 9688]\n",
      "loss: 498.723572  [  800/ 9688]\n",
      "loss: 1223.675903  [ 1600/ 9688]\n",
      "loss: 340.214111  [ 2400/ 9688]\n",
      "loss: 179.295532  [ 3200/ 9688]\n",
      "loss: 1085.368896  [ 4000/ 9688]\n",
      "loss: 1706.936523  [ 4800/ 9688]\n",
      "loss: 606.217529  [ 5600/ 9688]\n",
      "loss: 392.596893  [ 6400/ 9688]\n",
      "loss: 201.470871  [ 7200/ 9688]\n",
      "loss: 439.110474  [ 8000/ 9688]\n",
      "loss: 640.780029  [ 8800/ 9688]\n",
      "loss: 226.718521  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1205.460180 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 270.431396  [    0/ 9688]\n",
      "loss: 1196.061279  [  800/ 9688]\n",
      "loss: 57.108086  [ 1600/ 9688]\n",
      "loss: 944.441162  [ 2400/ 9688]\n",
      "loss: 172.890411  [ 3200/ 9688]\n",
      "loss: 1791.703247  [ 4000/ 9688]\n",
      "loss: 173.407379  [ 4800/ 9688]\n",
      "loss: 432.149384  [ 5600/ 9688]\n",
      "loss: 164.731247  [ 6400/ 9688]\n",
      "loss: 665.269714  [ 7200/ 9688]\n",
      "loss: 113.232368  [ 8000/ 9688]\n",
      "loss: 476.661957  [ 8800/ 9688]\n",
      "loss: 482.912231  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 1036.936832 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1221.194092  [    0/ 9688]\n",
      "loss: 187.404953  [  800/ 9688]\n",
      "loss: 1464.801270  [ 1600/ 9688]\n",
      "loss: 486.288788  [ 2400/ 9688]\n",
      "loss: 331.500610  [ 3200/ 9688]\n",
      "loss: 593.720886  [ 4000/ 9688]\n",
      "loss: 113.383713  [ 4800/ 9688]\n",
      "loss: 701.380615  [ 5600/ 9688]\n",
      "loss: 255.995850  [ 6400/ 9688]\n",
      "loss: 796.878906  [ 7200/ 9688]\n",
      "loss: 296.723846  [ 8000/ 9688]\n",
      "loss: 155.273010  [ 8800/ 9688]\n",
      "loss: 938.659302  [ 9600/ 9688]\n",
      "Test Error: Avg loss: 984.773336 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 154.594925  [    0/ 9688]\n",
      "loss: 831.386841  [  800/ 9688]\n",
      "loss: 246.479401  [ 1600/ 9688]\n",
      "loss: 38.424763  [ 2400/ 9688]\n",
      "loss: 143.519928  [ 3200/ 9688]\n"
     ]
    }
   ],
   "source": [
    "model.float()\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(training_data_loader, model, loss, optimizer)\n",
    "    test_loop(test_data_loader, model, loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"SEPT_TPS_{country}.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa70e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
